{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. SVM Hinge Loss Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hinge loss is defined as follows:\n",
    "<br><br>\n",
    "\\begin{equation}\n",
    "    L_{i} = \\sum_{j \\neq y_{i}} = \\max(0, s_{j} - s_{y_{i}} + \\Delta)\n",
    "\\end{equation}\n",
    "<br><br>\n",
    "$\\Delta$ is a margin value (it is an optimization hyperparameter). $s_{j}$ and $s_{y_{i}}$ are $j$th and $y_{i}$th class scores respectively for training example $x_{i}$. The class scores are computed with dot products $w_{j}^{T}x_{i}$ and $w_{y_{i}}^{T}x_{i}$.\n",
    "<br><br>\n",
    "The total loss to be minimized can be written as:\n",
    "<br><br>\n",
    "\\begin{equation}\n",
    "    L = \\frac{1}{N}\\sum_{i}^{N}L_{i} + \\frac{1}{2}\\lambda\\|W\\|_{2}^{2}\n",
    "\\end{equation}\n",
    "<br><br>\n",
    "where $\\lambda$ is a regularization hyperparameter and $\\frac{1}{2}$ is a constant for clean gradient computation. Intuitively, SVM wants score, $s_{y_{i}}=w_{y_{i}}^{T}x_{i}$ of the correct class $y_{i}$ to be greater than any other classes by at least the margin $\\Delta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Computing the gradient of Hinge Loss (Unvectorized ver.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compute the gradient of the loss function w.r.t $W$, we start off with the loss for each individual training sample $x_{i}$:<br><br>\n",
    "\\begin{equation}\n",
    "    L_{i} = \\sum_{j \\neq y_{i}} \\max(0, w_{j}^{T}x_{i} - w_{y_{i}}^{T}x_{i} + \\Delta)\n",
    "\\end{equation}\n",
    "<br><br>\n",
    "First, make sure to visualize that the wegiht matrix $W$ is of size ($D$, $C$) and our input matrix $X$ is of size ($N$, $D$) with $C$, $D$, $N$ represeting the number of classes, feature dimension, and number of train samples in $X$ respectively.\n",
    "<br><br>\n",
    "From the equation for $L_{i}$, we can see that we can compute the gradient of the loss by parts as follows:\n",
    "<br><br>\n",
    "1. When taking the gradient of loss w.r.t $w_{y_{i}}$:\n",
    "<br><br>\n",
    "\\begin{equation}\n",
    "    \\nabla_{w_{y_{i}}} L_{i} = \\sum_{j \\neq y_{i}} \\nabla_{w_{y_{i}}}\\max(0, w_{j}^{T}x_{i} - w_{y_{i}}^{T}x_{i} + \\Delta)\n",
    "\\end{equation}\n",
    "<br><br>\n",
    "When the hinge loss value of $w_{j}^{T}x_{i} - w_{y_{i}}^{T}x_{i} + \\Delta \\leq 0$, we can immediately see that $\\nabla_{w_{y_{i}}} L_{i} = \\nabla_{w_{y_{i}}} 0 = 0$. When it is greater than 0:\n",
    "<br><br>\n",
    "\\begin{equation}\n",
    "    \\nabla_{w_{y_{i}}} L_{i} = \\nabla_{w_{y_{i}}}(w_{j}^{T}x_{i} - w_{y_{i}}^{T}x_{i} + \\Delta) = -x_{i}\n",
    "\\end{equation}\n",
    "<br><br>\n",
    "Now, combining the two cases, we have the following result:\n",
    "<br><br>\n",
    "\\begin{equation}\n",
    "    \\nabla_{w_{y_{i}}} L_{i} = -(\\sum_{j \\neq y_{i}} \\mathbb{1}(w_{j}^{T}x_{i} - w_{y_{i}}^{T}x_{i} + \\Delta > 0))x_{i}\n",
    "\\end{equation}\n",
    "<br><br>\n",
    "where, $\\mathbb{1}()$ is an indicator function for given condition.\n",
    "<br><br>\n",
    "2. When taking the gradient of loss w.r.t $w_{j}$:\n",
    "<br><br>\n",
    "\\begin{equation}\n",
    "    \\nabla_{w_{j}} L_{i} = \\sum_{j \\neq y_{i}} \\nabla_{w_{j}}\\max(0, w_{j}^{T}x_{i} - w_{y_{i}}^{T}x_{i} + \\Delta)\n",
    "\\end{equation}\n",
    "<br><br>\n",
    "With similar computations, we can see that\n",
    "<br><br>\n",
    "\\begin{equation}\n",
    "    \\nabla_{w_{y_{i}}} L_{i} = \\mathbb{1}(w_{j}^{T}x_{i} - w_{y_{i}}^{T}x_{i} + \\Delta > 0)x_{i}\n",
    "\\end{equation}\n",
    "<br><br>\n",
    "As a final note, don't forget that hinge loss is not continuously differentiable. It is not differentiable at the point where hinge loss equals 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implementation of SVM loss and gradient computation (Naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_loss_naive(W, X, y, reg):\n",
    "  \"\"\"\n",
    "  Structured SVM loss function, naive implementation (with loops).\n",
    "\n",
    "  Inputs have dimension D, there are C classes, and we operate on minibatches\n",
    "  of N examples.\n",
    "\n",
    "  Inputs:\n",
    "  - W: A numpy array of shape (D, C) containing weights.\n",
    "  - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
    "  - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
    "    that X[i] has label c, where 0 <= c < C.\n",
    "  - reg: (float) regularization strength\n",
    "\n",
    "  Returns a tuple of:\n",
    "  - loss as single float\n",
    "  - gradient with respect to weights W; an array of same shape as W\n",
    "  \"\"\"\n",
    "  dW = np.zeros(W.shape) # initialize the gradient as zero\n",
    "\n",
    "  # compute the loss and the gradient\n",
    "  num_classes = W.shape[1]\n",
    "  num_train = X.shape[0]\n",
    "  loss = 0.0\n",
    "  for i in xrange(num_train):\n",
    "    # Get the class scores for ith train sample.\n",
    "    scores = X[i].dot(W)\n",
    "    # Set aside the correct class score for margin computation.\n",
    "    correct_class_score = scores[y[i]]\n",
    "    for j in xrange(num_classes):\n",
    "      if j == y[i]:\n",
    "        # Since hinge loss does not look at the correct class, we skip.\n",
    "        continue\n",
    "      margin = scores[j] - correct_class_score + 1 # note delta = 1\n",
    "      if margin > 0:\n",
    "        loss += margin\n",
    "        # Note W.shape = (D, C)\n",
    "        # Note X.shape = (N, D)\n",
    "        dW[:, y[i]] -= X[i, :]\n",
    "        dW[:, j] += X[i, :]\n",
    "  # Normalize the total loss with respect to num_train.\n",
    "  loss /= num_train\n",
    "  # Normalize the gradient with respect to num_train\n",
    "  dW /= num_train\n",
    "  # Add the regularization term to the total loss.\n",
    "  loss += 0.5 * reg * np.sum(W * W)\n",
    "  # Add the gradient of the regularization term to the gradient.\n",
    "  dW += reg * W\n",
    "    \n",
    "  return loss, dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Vectorized SVM hinge loss and gradient implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient derivation presented above is still applied in the vectorized implementation. Rather than computing the class score for each sample, we can compute the class scores for all training samples in $X$ by taking the dot product with weights $W$. Let's visualize the two matrices:\n",
    "<br><br>\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "    x_{11} & x_{12} & x_{13} & \\dots  & x_{1D} \\\\\n",
    "    x_{21} & x_{22} & x_{23} & \\dots  & x_{2D} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{N1} & x_{N2} & x_{N3} & \\dots  & x_{ND}\n",
    "\\end{bmatrix}, \n",
    "W =\n",
    "\\begin{bmatrix}\n",
    "    w_{11} & w_{21} & w_{31} & \\dots  & w_{C1} \\\\\n",
    "    w_{12} & w_{22} & w_{32} & \\dots  & w_{C2} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    w_{1D} & w_{2D} & w_{3D} & \\dots  & w_{CD}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "<br><br>\n",
    "Recall that each row of $X$ represents a training sample and each column of $W$ represents a weight vector for a class. Therefore, the dot product of the two:\n",
    "<br><br>\n",
    "$$\n",
    "XW =\n",
    "\\begin{bmatrix}\n",
    "    s_{11} & s_{12} & s_{13} & \\dots  & s_{1C} \\\\\n",
    "    s_{21} & s_{22} & s_{23} & \\dots  & s_{2C} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    s_{N1} & s_{N2} & s_{N3} & \\dots  & s_{NC}\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_loss_vectorized(W, X, y, reg):\n",
    "  \"\"\"\n",
    "  Structured SVM loss function, vectorized implementation.\n",
    "\n",
    "  Inputs and outputs are the same as svm_loss_naive.\n",
    "  \"\"\"\n",
    "  loss = 0.0\n",
    "  num_train = X.shape[0]\n",
    "  dW = np.zeros(W.shape) # initialize the gradient as zero\n",
    "  scores = X.dot(W)\n",
    "  correct_scores = scores[np.arange(scores.shape[0]), y]\n",
    "  margin = np.maximum(0, scores - np.matrix(correct_scores).T + 1)\n",
    "  margin[np.arange(num_train), y] = 0\n",
    "  loss = np.mean(np.sum(margin, axis=1))\n",
    "  loss += 0.5 * reg * np.sum(W * W)\n",
    "  \n",
    "  # As we have derived from the previous section,\n",
    "  # We need an indicator matrix for margins that are greater than 0.\n",
    "  indicators = margin\n",
    "  indicators[margin > 0] = 1\n",
    "  # For the gradient of the loss w.r.t correct class weight,\n",
    "  # we must sum 1's across the columns of each row.\n",
    "  row_sum = np.sum(indicators, axis=1)\n",
    "  indicators[np.arange(num_train), y] = -row_sum.T\n",
    "  dW = X.T.dot(indicators)\n",
    "  dW /= num_train\n",
    "  dW += reg * W\n",
    "\n",
    "  return loss, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
