{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. SVM Hinge Loss Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hinge loss is defined as follows:\n",
    "<br><br>\n",
    "\\begin{equation}\n",
    "    L_{i} = \\sum_{j \\neq y_{i}} = \\max(0, s_{j} - s_{y_{i}} + \\Delta)\n",
    "\\end{equation}\n",
    "<br><br>\n",
    "$\\Delta$ is a margin value (it is an optimization hyperparameter). $s_{j}$ and $s_{y_{i}}$ are $j$th and $y_{i}$th class scores respectively for training example $x_{i}$. The class scores are computed with dot products $w_{j}^{T}x_{i}$ and $w_{y_{i}}^{T}x_{i}$.\n",
    "<br><br>\n",
    "The total loss to be minimized can be written as:\n",
    "<br><br>\n",
    "\\begin{equation}\n",
    "    L = \\frac{1}{N}\\sum_{i}^{N}L_{i} + \\frac{1}{2}\\lambda\\|W\\|_{2}^{2}\n",
    "\\end{equation}\n",
    "<br><br>\n",
    "where $\\lambda$ is a regularization hyperparameter and $\\frac{1}{2}$ is a constant for clean gradient computation. Intuitively, SVM wants score, $s_{y_{i}}=w_{y_{i}}^{T}x_{i}$ of the correct class $y_{i}$ to be greater than any other classes by at least the margin $\\Delta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Computing the gradient of Hinge Loss (Unvectorized ver.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compute the gradient of the loss function w.r.t $W$, we start off with the loss for each individual training sample $x_{i}$:<br><br>\n",
    "\\begin{equation}\n",
    "    L_{i} = \\sum_{j \\neq y_{i}} \\max(0, w_{j}^{T}x_{i} - w_{y_{i}}^{T}x_{i} + \\Delta)\n",
    "\\end{equation}\n",
    "<br><br>\n",
    "First, make sure to visualize that the wegiht matrix $W$ is of size ($D$, $C$) and our input matrix $X$ is of size ($N$, $D$) with $C$, $D$, $N$ represeting the number of classes, feature dimension, and number of train samples in $X$ respectively.\n",
    "<br><br>\n",
    "From the equation for $L_{i}$, we can see that we can compute the gradient of the loss by parts as follows:\n",
    "<br><br>\n",
    "1. When taking the gradient of loss w.r.t $w_{y_{i}}$:\n",
    "<br><br>\n",
    "\\begin{equation}\n",
    "    \\nabla_{w_{y_{i}}} L_{i} = \\sum_{j \\neq y_{i}} \\nabla_{w_{y_{i}}}\\max(0, w_{j}^{T}x_{i} - w_{y_{i}}^{T}x_{i} + \\Delta)\n",
    "\\end{equation}\n",
    "<br><br>\n",
    "When the hinge loss value of $w_{j}^{T}x_{i} - w_{y_{i}}^{T}x_{i} + \\Delta \\leq 0$, we can immediately see that $\\nabla_{w_{y_{i}}} L_{i} = \\nabla_{w_{y_{i}}} 0 = 0$. When it is greater than 0:\n",
    "<br><br>\n",
    "\\begin{equation}\n",
    "    \\nabla_{w_{y_{i}}} L_{i} = \\nabla_{w_{y_{i}}}(w_{j}^{T}x_{i} - w_{y_{i}}^{T}x_{i} + \\Delta) = -x_{i}\n",
    "\\end{equation}\n",
    "<br><br>\n",
    "Now, combining the two cases, we have the following result:\n",
    "<br><br>\n",
    "\\begin{equation}\n",
    "    \\nabla_{w_{y_{i}}} L_{i} = -(\\sum_{j \\neq y_{i}} \\mathbb{1}(w_{j}^{T}x_{i} - w_{y_{i}}^{T}x_{i} + \\Delta > 0))x_{i}\n",
    "\\end{equation}\n",
    "<br><br>\n",
    "where, $\\mathbb{1}()$ is an indicator function for given condition.\n",
    "<br><br>\n",
    "2. When taking the gradient of loss w.r.t $w_{j}$:\n",
    "<br><br>\n",
    "\\begin{equation}\n",
    "    \\nabla_{w_{j}} L_{i} = \\sum_{j \\neq y_{i}} \\nabla_{w_{j}}\\max(0, w_{j}^{T}x_{i} - w_{y_{i}}^{T}x_{i} + \\Delta)\n",
    "\\end{equation}\n",
    "<br><br>\n",
    "With similar computations, we can see that\n",
    "<br><br>\n",
    "\\begin{equation}\n",
    "    \\nabla_{w_{y_{i}}} L_{i} = \\mathbb{1}(w_{j}^{T}x_{i} - w_{y_{i}}^{T}x_{i} + \\Delta > 0)x_{i}\n",
    "\\end{equation}\n",
    "<br><br>\n",
    "As a final note, don't forget that hinge loss is not continuously differentiable. It is not differentiable at the point where hinge loss equals 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Computing the vectorized"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
